{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759daefc",
   "metadata": {},
   "source": [
    "# Market-implied stock price PDFs: risk-neutral and physical\n",
    "\n",
    "Option prices embed information about the markets expectation of future performance of the underlying asset.\n",
    "The set of European option prices across strikes for a given maturity $T$ implies a risk-neutral probability density function (details below) of the price $S_T$ of the underlying asset at the maturity.\n",
    "\n",
    "This density reflects the distribution consistent with current option prices under the pricing measure that discounts payoffs at the risk-free rate.\n",
    "It also reflects both how the market expects the underlying to move (although it is important to note that the pdf we obtain is NOT simply the market's prediction of where the stock is likely to move), as well as how the market prices risk across the various outcomes.\n",
    "\n",
    "This project uses several years of daily SPX option chain data to extract the market-implied risk-neutral pdfs for options with 1 day maturities, 7 day maturities, and 28 day maturities.\n",
    "Using the Breeden-Litzenberger relation, which says that the implied pdf is given by the second partial derivative of price at maturity with respect to strike:\n",
    "$$ f_Q(K) = e^{rT} \\frac{\\partial^2 C(K, T)}{\\partial K^2},$$\n",
    "we numerically recover the pdf.\n",
    "Since strikes come in discrete increments, we require some numerical techniques to interpolate.\n",
    "We also need to smooth the data enough to take a second derivative.\n",
    "\n",
    "I then discuss and compare ways of recovering the real-world probability distribution from the risk-neutral one.\n",
    "\n",
    "There are two main goals of this project:\n",
    "1. Compare the risk-neutral pdf to the realized price distribution.\n",
    "2. Test a simple way of estimating the real-world density function from the risk-neutral density.\n",
    "\n",
    "The secondary goal is to give exposition of the theory and background needed to understand this topic so that a wider audience may find it useful and engaging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673756d",
   "metadata": {},
   "source": [
    "## Part 1: risk-neutral distributions\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "#### Risk-neutral probability measure\n",
    "\n",
    "We start with almost the simplest possible case: a market that will evolve over only one timestep.\n",
    "Time $t=0$ is the starting point, and time $t=1$ is the final step in the evolution of the market.\n",
    "At $t=1$, the world can be in one of $n$ possible states\n",
    "$$ \\Omega = \\{\\omega_1, \\dots, \\omega_n\\}$$\n",
    "with real-world probabilities $P(\\omega_i) \\geq 0$, $\\sum_i P(\\omega_i) = 1$.\n",
    "\n",
    "Suppose our market has one traded asset $S$, worth $\\$100$ at $t=0$ and a risk-free account with interest rate $r$.\n",
    "Suppose also that for some set $U \\subset \\Omega$ of states of the world at $t=1$, $S$ will go up to $S_0 \\cdot u$, and for the complement $D = \\Omega \\setminus U$, $S$ will go down to $S_0 \\cdot d$, for some factors $u,d$ with $0 < d < 1 < u$.\n",
    "\n",
    "There is some real-world probability $P(U)$ that $S$ will go up, and some real probability $P(D)$ that $S$ will go down.\n",
    "These probabilities are of course unknowable, but it turns out that they are not necessary for the risk-neutral probability measure.\n",
    "\n",
    "Now if $e^r < .9$, then there is an arbitrage scheme: borrow $S_0$ dollars at $t=0$ and buy one unit of $S$.\n",
    "Then regardless of whether the asset goes up or down, the value at $t=1$ would be higher than $S_0 e^r$, which is the amount one would owe at the end.\n",
    "\n",
    "Similarly, if $e^r > 1.1$, one could short the stock and invest the proceeds into the risk-free account.\n",
    "\n",
    "The *no-arbitrage condition* $.9 \\leq e^r \\leq 1.1$ implies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### The Breeden-Litzenberger Formula\n",
    "Recall that the fair value of a call option with time $\\tau$ until expiry is the present value of the expected payoff.\n",
    "If $f_Q$ denotes the PDF of the stock prices $S_T$ of some stock $S$ at time $T$ (with respect to the risk-neutral measure $Q$), this is\n",
    "$$\n",
    "\\begin{align*}\n",
    "C(K, \\tau) &= e^{-rt} \\mathbb E_Q(\\max\\{S_T - K, 0\\}) \\\\\n",
    "&= e^{-rt} \\int_0^\\infty \\max\\{x - K, 0\\} f_Q(x) \\, d x \\\\\n",
    "&= e^{-rt} \\int_K^\\infty (x - K) f(x) \\, dx.\n",
    "\\end{align*}\n",
    "$$\n",
    "To derive the Breeden-Litzenberger formula, we just need to invert this to find $f$ as a function of $C$.\n",
    "Taking the partial derivative with respect to $K$ gives\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial K} &= - e^{-rt} \\int_{K}^\\infty f(x) \\, dx \\\\\n",
    "&= e^{-rt} \\left( \\int_{-\\infty}^K f(x) \\, dx - 1 \\right),\n",
    "\\end{align*}\n",
    "$$\n",
    "where in the second step we use the fact that $\\int_{\\R} f(x) \\, dx = 1$.\n",
    "But now we can find $f$ by taking one more derivative.\n",
    "Doing this and rearranging, we see that\n",
    "$$f(K) = e^{rt} \\frac{\\partial^2 C}{\\partial K^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68507dd4",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### The data\n",
    "\n",
    "I obtained my options chain data from OptionsDX.\n",
    "I have EOD options chain data for SPX for years 2020 through 2023.\n",
    "I filtered for options with 1 day maturities, 7 day maturities, and 28 day maturities, so that I can compare all the methods across a range of expiration lengths.\n",
    "See `processing.ipynb` in the `data` directory for the code and more explanation of data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f408e7",
   "metadata": {},
   "source": [
    "First I'll import the data and get it into the dataframes that I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0e7875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:58:47.541464Z",
     "iopub.status.busy": "2025-11-03T23:58:47.541230Z",
     "iopub.status.idle": "2025-11-03T23:58:50.903276Z",
     "shell.execute_reply": "2025-11-03T23:58:50.902301Z"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from black_scholes import *     # utility functions related to the Black-Scholes formula\n",
    "\n",
    "from scipy.interpolate import CubicSpline # for cubic interpolation with smoothing\n",
    "from scipy.interpolate import make_splrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_years = [\"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "spx_dfs = [\n",
    "    pd.read_csv(f\"data/spx_options_{year}.csv\")\n",
    "    for year in data_years\n",
    "]\n",
    "\n",
    "spx_df = pd.concat(spx_dfs, ignore_index=True).dropna()\n",
    "\n",
    "\n",
    "\n",
    "numeric_cols = ['call_iv', 'call_last', 'put_last'] # TODO: and some more\n",
    "for col in numeric_cols:\n",
    "    spx_df[col] = pd.to_numeric(spx_df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"This datatframe contains all the data that we'll use for this project.\")\n",
    "display(spx_df)\n",
    "\n",
    "spx_1dte = spx_df[spx_df['tte_unix'] == 60*60*24]\n",
    "spx_7dte = spx_df[spx_df['tte_unix'] == 60*60*24*7]\n",
    "spx_28dte = spx_df[spx_df['tte_unix'] == 60*60*24*28]\n",
    "\n",
    "# create a collection of dataframes, one for each day\n",
    "spx_1dte_dfs = [\n",
    "    spx_1dte[spx_1dte['quote_unix'] == utime]\n",
    "    for utime in spx_1dte['quote_unix'].unique()\n",
    "]\n",
    "\n",
    "spx_7dte_dfs = [\n",
    "    spx_7dte[spx_7dte['quote_unix'] == utime]\n",
    "    for utime in spx_7dte['quote_unix'].unique()\n",
    "]\n",
    "\n",
    "spx_28dte_dfs = [\n",
    "    spx_28dte[spx_28dte['quote_unix'] == utime]\n",
    "    for utime in spx_28dte['quote_unix'].unique()\n",
    "]\n",
    "\n",
    "print(\"This dataframe corresponds to all the options on a given day with a given expiry date.\")\n",
    "display(spx_1dte_dfs[500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f186cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# pick a random option chain to display (from the 7dte options)\n",
    "df = spx_7dte_dfs[random.randint(0,len(spx_7dte_dfs)-1)].copy()\n",
    "expire_date = pd.to_datetime(df['expire_unix'].mode()[0], unit=\"s\").date()\n",
    "plt.title(f\"Strike vs mid price for a 7DTE option expiring on {expire_date}\")\n",
    "\n",
    "ax.plot(df['strike'], df['call_mid'], color=\"#1F1F84\", linewidth=2, label=f\"Strike vs mid price\")\n",
    "ax.vlines(df['underlying_last'].mean(), ymin=0, ymax=df['call_mid'].max(), color=\"#5C5C5C\", linestyle=\"--\", linewidth=1, label=\"SPX Spot price\")\n",
    "\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.set_xlabel(\"Strike price\")\n",
    "ax.set_ylabel(\"Mid price\");\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b4f58",
   "metadata": {},
   "source": [
    "Since the scale of this plot is large, this curve may appear smooth even when it contains small irregularities.\n",
    "Extracting the implied probability density requires computing second derivatives, which will amplify any irregularities.\n",
    "Due to this, we need to smooth this curve.\n",
    "Also due to this, we should not try interpolating/smoothing directly on this data; the second derivative will be highly unstable.\n",
    "\n",
    "A more robust approach is to convert the prices to implied volatilities using Black-Scholes, perform the interpolation in volatility space, and then convert back to prices.\n",
    "In fact, we'll do one more step: after we obtain implied volatility as a function of strike price, we'll convert it to a function of log-moneyness (roughly, this is just the log of the strike price), *then* do the smoothing, and then convert back.\n",
    "\n",
    "The example below illustrates the problem with trying to smooth prices directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "# Assume df already exists with columns 'strikes' and 'call_mid'\n",
    "\n",
    "# 1. Extract and clean data\n",
    "K = df[\"strike\"].to_numpy()\n",
    "C_mid = df[\"call_mid\"].to_numpy()\n",
    "\n",
    "mask = np.isfinite(K) & np.isfinite(C_mid)\n",
    "K = K[mask]\n",
    "C_mid = C_mid[mask]\n",
    "\n",
    "order = np.argsort(K)\n",
    "K = K[order]\n",
    "C_mid = C_mid[order]\n",
    "\n",
    "smoothing = 0.05 * len(K)\n",
    "\n",
    "price_spline = UnivariateSpline(K, C_mid, k=3, s=smoothing)\n",
    "\n",
    "K_fine = np.linspace(K.min(), K.max(), 1000)\n",
    "C_smooth = price_spline(K_fine)\n",
    "\n",
    "dC_dK = np.gradient(C_smooth, K_fine)\n",
    "d2C_dK2 = np.gradient(dC_dK, K_fine)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "axes[0].scatter(K, C_mid, s=10, alpha=0.6, label=\"mid prices\")\n",
    "axes[0].plot(K_fine, C_smooth, label=\"spline on price\", linewidth=1.5)\n",
    "axes[0].set_ylabel(\"Call price\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"Naive smoothing in price space\")\n",
    "\n",
    "axes[1].plot(K_fine, dC_dK, linewidth=1.5)\n",
    "axes[1].set_ylabel(\"C'(K)\")\n",
    "\n",
    "axes[2].plot(K_fine, d2C_dK2, linewidth=1.5)\n",
    "axes[2].axhline(0.0, color=\"black\", linestyle=\"--\", linewidth=0.8)\n",
    "axes[2].set_ylabel(\"C''(K)\")\n",
    "axes[2].set_xlabel(\"Strike\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd8adc",
   "metadata": {},
   "source": [
    "In the following cell, we define functions that convert the price data to implied volatility, perform a smoothed interpolation, convert back to price, and then compute the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from scipy.interpolate import make_splrep, interp1d\n",
    "\n",
    "def compute_implied_vols(strikes, prices, spot, T, option_type, r=0.0):\n",
    "    strikes = np.asarray(strikes)\n",
    "    prices = np.asarray(prices)\n",
    "\n",
    "    iv_list = []\n",
    "    strikes_list = []\n",
    "\n",
    "    for k, c in zip(strikes, prices):\n",
    "        try:\n",
    "            iv = bs_implied_volatility(\n",
    "                option_type=option_type,\n",
    "                market_price=c,\n",
    "                S0=spot,\n",
    "                K=k,\n",
    "                t=T,\n",
    "                r=r,\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if iv is None or not np.isfinite(iv):\n",
    "            continue\n",
    "\n",
    "        strikes_list.append(k)\n",
    "        iv_list.append(iv)\n",
    "\n",
    "    strikes_valid = np.array(strikes_list)\n",
    "    iv_valid = np.array(iv_list)\n",
    "\n",
    "    # plt.plot(strikes_valid, iv_valid)\n",
    "    # plt.title('strikes_valid vs iv_valid from compute_implied_vols()')\n",
    "    # plt.show()\n",
    "\n",
    "    order = np.argsort(strikes_valid)\n",
    "    return strikes_valid[order], iv_valid[order]\n",
    "\n",
    "def fit_iv_logmoneyness(strikes, iv, forward, smoothing_factor=None):\n",
    "    \"\"\"\n",
    "    Fit a smooth implied vol curve as a function of log-moneyness lm = log(strike/F),\n",
    "    enforcing positivity by fitting log(iv) and exponentiating.\n",
    "\n",
    "    Returns iv as a function of strike and the domain of the fitted curve.\n",
    "    \"\"\"\n",
    "    strikes = np.asarray(strikes)\n",
    "    iv = np.asarray(iv)\n",
    "\n",
    "    lm_vals = np.log(strikes/forward)\n",
    "\n",
    "    iv = np.maximum(iv, 1e-4)\n",
    "    log_iv = np.log(iv)\n",
    "\n",
    "    # sort by lm for a well-behaved spline\n",
    "    order = np.argsort(lm_vals)\n",
    "    lm_sorted = lm_vals[order]\n",
    "    log_iv_sorted = log_iv[order]\n",
    "    strikes_sorted = strikes[order]\n",
    "\n",
    "    if smoothing_factor is None:\n",
    "        n = len(lm_vals)\n",
    "        smoothing_factor = 0.01 * n\n",
    "\n",
    "    # spline in log-moneyness space\n",
    "    spline_log_iv = make_splrep(\n",
    "        lm_sorted,\n",
    "        log_iv_sorted,\n",
    "        k=3,\n",
    "        s=smoothing_factor,\n",
    "    )\n",
    "\n",
    "    # --- function of STRIKE, wrapping log-moneyness ---\n",
    "    def iv_of_strike(K):\n",
    "        K = np.asarray(K)\n",
    "        lm = np.log(K / forward)\n",
    "        log_iv_val = spline_log_iv(lm)      # spline is defined in lm-space\n",
    "        return np.exp(log_iv_val)     \n",
    "\n",
    "    # spline_log_iv = make_splrep(lm_vals, log_iv, k=3, s=smoothing_factor)\n",
    "\n",
    "    # iv_of_lm = lambda lm: np.exp(spline_log_iv(lm))\n",
    "\n",
    "    K_grid = np.linspace(strikes_sorted.min(), strikes_sorted.max(), 500)\n",
    "    # plt.scatter(strikes_sorted, iv[order], s=10, label=\"data\")\n",
    "    # plt.plot(K_grid, iv_of_strike(K_grid), label=\"spline IV(K)\")\n",
    "    # plt.title(\"Strike vs IV (smoothed via log-moneyness)\")\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # plt.plot(lm_vals, iv_of_lm(lm_vals))\n",
    "    # plt.title(\"log moneyness vs iv(lm) smoothed from fit_iv_logmoneyness()\")\n",
    "    # plt.show()\n",
    "\n",
    "    return iv_of_strike, K_grid\n",
    "    \n",
    "\n",
    "def smooth_option_curve(strikes, option_prices, spot, T, option_type, r=0, forward=None, n_grid=500, smoothing_factor=None):\n",
    "    strikes = np.asarray(strikes)\n",
    "    option_prices = np.asarray(option_prices)\n",
    "\n",
    "    # in a few dataframes there are large gaps here\n",
    "\n",
    "    order = np.argsort(strikes)\n",
    "    strikes = strikes[order]\n",
    "    option_prices = option_prices[order]\n",
    "\n",
    "    diffs = np.diff(strikes)\n",
    "    typical_step = np.median(diffs)  # robust estimate\n",
    "\n",
    "    # Define a uniform strike grid between min and max\n",
    "    strike_fine = np.arange(strikes.min(), strikes.max() + typical_step, typical_step)\n",
    "\n",
    "    # linearly interpolate option prices\n",
    "    interp_func = interp1d(strikes, option_prices, kind='linear', fill_value=\"extrapolate\")\n",
    "    option_prices_interp = interp_func(strike_fine)\n",
    "\n",
    "    strikes = strike_fine\n",
    "    option_prices = option_prices_interp\n",
    " \n",
    "    # plt.plot(strikes, option_prices)\n",
    "    # plt.scatter(strikes, option_prices)\n",
    "    # plt.title(\"strikes vs option prices from smooth_option_curve after interpolation\")\n",
    "    # plt.show()\n",
    "\n",
    "    # calculate iv on observed strikes\n",
    "    strikes_obs, iv_obs = compute_implied_vols(\n",
    "        strikes=strikes,\n",
    "        prices=option_prices,\n",
    "        spot=spot,\n",
    "        T=T,\n",
    "        option_type=option_type,\n",
    "        r=r,\n",
    "    )\n",
    "\n",
    "    # plt.plot(strikes_obs, iv_obs)\n",
    "    # plt.title(\"observed strikes vs obs iv\")\n",
    "    # plt.show()\n",
    "\n",
    "    if forward is None:\n",
    "        forward = spot * np.exp(r * T)\n",
    "\n",
    "    # fit IV(lm)\n",
    "    iv_of_strike, strike_grid = fit_iv_logmoneyness(\n",
    "        strikes=strikes_obs,\n",
    "        iv=iv_obs,\n",
    "        forward=forward,\n",
    "        smoothing_factor=smoothing_factor\n",
    "    )\n",
    "\n",
    "    # plt.plot(strike_grid, iv_of_strike(strike_grid))\n",
    "    # plt.title(\"iv(lm) from smooth_option_curve\")\n",
    "    # plt.show()\n",
    "\n",
    "    # make a fine grid in lm and map back to strike\n",
    "#     lm_min, lm_max = lm_vals.min(), lm_vals.max()\n",
    "#     lm_grid = np.linspace(lm_min, lm_max, n_grid)\n",
    "#     strike_grid = forward * np.exp(lm_grid)\n",
    "\n",
    "#     def iv_of_strike(strike):\n",
    "#         strike = np.asarray(strike)\n",
    "#         lm = np.log(strike / forward)\n",
    "#         return iv_of_lm(lm)\n",
    "\n",
    "#     iv_smooth = iv_of_lm(lm_grid)\n",
    "# V    iv_smooth = np.maximum(iv_smooth, 1e-5)\n",
    "\n",
    "    # plt.plot(lm_grid, iv_smooth)\n",
    "    # plt.title(\"lm_grid vs iv_smooth\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.plot(strike_grid, iv_of_strike(strike_grid))\n",
    "    # plt.title(\"strike vs iv smoothed from smooth_option_curve()\")\n",
    "    # plt.show()\n",
    "\n",
    "    # convert back to prices via Black–Scholes\n",
    "    price_smooth = np.array([\n",
    "        bs_price(\n",
    "            S0=spot,\n",
    "            K=strike_grid[i],\n",
    "            sigma=iv_of_strike(strike_grid)[i],\n",
    "            t=T,\n",
    "            r=r,\n",
    "            option_type=option_type,\n",
    "        )\n",
    "        for i in range(len(strike_grid))\n",
    "    ])\n",
    "    \n",
    "    return strike_grid, price_smooth, iv_of_strike\n",
    "\n",
    "def create_pdf_from_df(df: pd.DataFrame, option_type):\n",
    "\n",
    "    strike_grid, price_smooth, iv_of_strike = smooth_option_curve(\n",
    "        strikes = df['strike'],\n",
    "        option_prices = df[f'{option_type}_mid'],\n",
    "        spot = df['underlying_last'].mean(), # this column should be constant but we'll take the mean anyway\n",
    "        T = df['tte_years'].iloc[0],\n",
    "        option_type = option_type,\n",
    "    )\n",
    "    \n",
    "    f = np.gradient(price_smooth, strike_grid)\n",
    "    f_2 = np.gradient(f, strike_grid)\n",
    "\n",
    "\n",
    "    # ensure nonnegative and normalize\n",
    "    f_2 = np.clip(f_2, 0, None)\n",
    "    area = np.trapezoid(f_2, strike_grid)\n",
    "    # print(f\"area of f_2: {area}\")\n",
    "    if area > 0:\n",
    "        f_2 /= area\n",
    "\n",
    "\n",
    "    # return strikes_fine, bs_prices_fine, CubicSpline(strikes_fine, f_2)\n",
    "    return strike_grid, price_smooth, CubicSpline(strike_grid, f_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f75ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    dfs = {\n",
    "        \"1DTE\": spx_1dte_dfs,\n",
    "        \"7DTE\": spx_7dte_dfs,\n",
    "        \"28DTE\": spx_28dte_dfs,\n",
    "    }\n",
    "\n",
    "    df_type = random.choice([\"1DTE\", \"7DTE\", \"28DTE\"])\n",
    "    df = dfs[df_type][random.randint(0, len(dfs[df_type]))].copy()\n",
    "\n",
    "    # for i, search_df in enumerate(spx_1dte_dfs):\n",
    "    #     if \"2021-11-29 16:00:00\" in search_df[\"quote_time\"].values:\n",
    "    #         print(f\"Found in dataframe {i}\")\n",
    "    #         df = search_df\n",
    "    #         break\n",
    "\n",
    "    # df = spx_1dte_dfs[14]\n",
    "\n",
    "\n",
    "    # df = spx_28dte_dfs[random.randint(0, len(spx_1dte_dfs))].copy()\n",
    "\n",
    "    spot_price = df['underlying_last'].mean()   # I am taking the mean just in case, but the underlying_last column should contain only one number\n",
    "\n",
    "    option_type = [\"call\", \"put\"][random.randint(0,1)]\n",
    "\n",
    "    # strikes_fine, bs_prices_fine, f_2 = create_pdf(\n",
    "    #     option_type=option_type, # TODO: also do put options\n",
    "    #     strikes=df['strike'],\n",
    "    #     option_prices=df[f'{option_type}_mid'],\n",
    "    #     underlying_prices=df[\"underlying_last\"],\n",
    "    #     tte_years=tte_years,\n",
    "    #     spot_price=spot_price\n",
    "    # )\n",
    "\n",
    "    # plt.plot(df['strike'], df['call_mid'])\n",
    "    # plt.title(\"strike vs mid\")\n",
    "    # plt.show()\n",
    "\n",
    "    strike_grid, price_smooth, iv_smooth = smooth_option_curve(\n",
    "        strikes=df['strike'],\n",
    "        option_prices=df['call_mid'],\n",
    "        spot=spot_price,\n",
    "        T=df['tte_years'].iloc[0],\n",
    "        option_type=\"call\",\n",
    "        r=0,\n",
    "        n_grid=500,\n",
    "    )\n",
    "\n",
    "    # plt.plot(strike_grid, price_smooth)\n",
    "    # plt.title(\"strike vs price NEW smoothed\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.plot(strike_grid, iv_smooth(strike_grid))\n",
    "    # plt.title(\"strike vs iv NEW smoothed\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.plot(df['strike'], df['call_iv'])\n",
    "    # plt.title(\"strike vs iv\")\n",
    "    # plt.show()\n",
    "    \n",
    "    strikes_fine, bs_prices_fine, f_2 = create_pdf_from_df(df, 'call')\n",
    "\n",
    "    # plt.plot(strikes_fine, bs_prices_fine)\n",
    "    # plt.title(\"strikes fine vs bs prices fine\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(strikes_fine, bs_prices_fine)\n",
    "    ax1.set_xlabel('Strike')\n",
    "    ax1.set_ylabel('Price as a function of strike (smoothed)', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(strikes_fine, f_2(strikes_fine), color='tab:red', label='pdf')\n",
    "    ax2.set_ylabel('f(K)', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # print(f_2(strikes_fine).min())\n",
    "    ax1.vlines(x=spot_price, ymin=bs_prices_fine.min(), ymax=bs_prices_fine.max(), color=\"#777777\", linestyles=\"--\", label=\"Spot price\")\n",
    "    # ax1.vlines(x=spot_price * np.exp(0.5 * tte_years), ymin=bs_prices_fine.min(), ymax=bs_prices_fine.max(), color=\"#777777\", linestyles=\"--\", label=\"Spot price\")\n",
    "    plt.title(f\"{df_type} {option_type} implied option price pdf on {pd.to_datetime(df['expire_unix'].iloc[0], unit=\"s\").date()}\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76486270",
   "metadata": {},
   "source": [
    "Now we need to do a little numeric differentiation to apply the Breeden-Litzenberger formula, which we recall from the discussion above says that\n",
    "$$f(K) = e^{rt} \\frac{\\partial^2 C}{\\partial K^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb5057",
   "metadata": {},
   "source": [
    "## Comparing to market data\n",
    "\n",
    "The method that we'll use to evaluate the accuracy of the predictions is called the **continuous ranked probability score.**\n",
    "It is defined by\n",
    "$$ C(D, y) = \\int_{\\R} (F_D(x) - \\mathbf 1_{x \\geq y})^2 \\, dx, $$\n",
    "where\n",
    "- $F_D$ is the cumulative distribution function of the forecasted distribution $D$,\n",
    "- $\\mathbf 1_{x \\geq y}$ is the indicator function, which is 1 if $x \\geq y$ and 0 otherwise, and\n",
    "- $y \\in \\R$ is the observation.\n",
    "\n",
    "I will evaluate this integral numerically to assign a score to the predicted pdfs.\n",
    "It is not hard to see by looking at the definition that a lower score is better, with 0 being the lowest possible score, which can only be achieved by the distribution that assigns probability 1 to the observed outcome, and 0 to all other outcomes.\n",
    "In general, this score rewards some combination of both precision and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d2058",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We'll start by downloading the SPX data for the same range that we have option data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_day = pd.to_datetime(spx_df['quote_unix'].min(), unit=\"s\")\n",
    "latest_day = pd.to_datetime(spx_df['expire_unix'].max(), unit=\"s\")\n",
    "\n",
    "spx_data = yf.download(\"^SPX\", start=earliest_day, end=latest_day, auto_adjust=True)\n",
    "display(spx_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab17e2",
   "metadata": {},
   "source": [
    "Now we implement the CRPS function described above, and a function that will calculate the CRPS given a dataframe of option data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d12347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crps(D_pdf, y, min_input, max_input, n_samples=500):\n",
    "    \"\"\"\n",
    "    Compute the Continuous Ranked Probability Score (CRPS) for a given predictive\n",
    "    distribution and observation, when D_pdf is a *pdf* on [min_input, max_input].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D_pdf : callable\n",
    "        Probability density function f(x) of the predictive distribution.\n",
    "        It must be (approximately) supported on [min_input, max_input].\n",
    "    y : float\n",
    "        Observed value.\n",
    "    min_input : float\n",
    "        Lower bound of the domain where the pdf is non-negligible.\n",
    "    max_input : float\n",
    "        Upper bound of the domain where the pdf is non-negligible.\n",
    "    n_samples : int, optional\n",
    "        Number of grid points used for numerical integration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Approximate CRPS(F, y).\n",
    "    \"\"\"\n",
    "    # Create a grid on the domain\n",
    "    z = np.linspace(min_input, max_input, int(n_samples))\n",
    "    dz = z[1] - z[0]\n",
    "\n",
    "    # Evaluate the pdf on the grid and clip in case of numerical errors\n",
    "    f = np.asarray(D_pdf(z), dtype=float)\n",
    "    f = np.clip(f, 0.0, np.inf)\n",
    "\n",
    "    # # Normalize f - actually I think this is always taken care of in other steps\n",
    "    # norm = np.trapezoid(f, z)\n",
    "    # if norm <= 0:\n",
    "    #     raise ValueError(\"PDF integrates to zero or negative over the given bounds.\")\n",
    "    # f /= norm\n",
    "\n",
    "    # Build the CDF via cumulative sum\n",
    "    F = np.cumsum(f) * dz\n",
    "    # Clip for numerical safety so CDF stays in [0, 1]\n",
    "    F = np.clip(F, 0.0, 1.0)\n",
    "\n",
    "    # CRPS integral: \\int (F(z) - 1_{z >= y})^2 dz\n",
    "    indicator = (z >= y).astype(float)\n",
    "    integrand = (F - indicator) ** 2\n",
    "    crps_val = np.trapezoid(integrand, z)\n",
    "\n",
    "    return float(crps_val)\n",
    "\n",
    "\n",
    "def evaluate_rn_option_prediction_from_df(df: pd.DataFrame, option_type):\n",
    "    \"\"\"\n",
    "    Given a dataframe, construct the risk-neutral implied pdf of option prices at the expiry date of the options in the frame.\n",
    "    Then look up the actual price of SPX at close on the day of expiration and use CRPS to score the prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    expiry_date = str(pd.to_datetime(df['expire_unix'].iloc[0], unit=\"s\").date())\n",
    "    spx_price_on_expiry_date = spx_data.loc[expiry_date, \"Close\"].iloc[0]\n",
    "\n",
    "    strikes_fine, bs_prices_fine, f_2 = create_pdf_from_df(df, option_type)\n",
    "    \n",
    "    return crps(f_2, spx_price_on_expiry_date, strikes_fine.min(), strikes_fine.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283878d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Calculating CRPS scores...\")\n",
    "for df_type in [spx_1dte_dfs, spx_7dte_dfs, spx_28dte_dfs]:\n",
    "    c = 0\n",
    "    p = 0\n",
    "    for df in df_type:\n",
    "        \n",
    "        call_score = evaluate_rn_option_prediction_from_df(df, 'call')\n",
    "        put_score = evaluate_rn_option_prediction_from_df(df, 'put')\n",
    "        c += call_score\n",
    "        p += put_score\n",
    "\n",
    "    print(f\"Average CRPS for {int(df_type[0]['dte'].iloc[0])} DTE calls: {c / len( df_type ) :.2f}\")\n",
    "    print(f\"Average CRPS for {int(df_type[0]['dte'].iloc[0])} DTE puts: {p / len( df_type ) :.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb22f56",
   "metadata": {},
   "source": [
    "By themselves these numbers are too abstract to be useful; it isn't clear what constitutes a good or bad score.\n",
    "We can get some intuition for the meaning of the scores with the following code.\n",
    "This code calculates the hypothetical scenario in which the stock price at the expiry of the option turns out to be the price that maximizes the density function for the market-implied probability distribution.\n",
    "\n",
    "We'll also find these numbers useful for comparison in the next section where we test a method of converting the risk-neutral density to real-world density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa779ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "print(\"Calculating CRPS scores in the hypothetical case that all predictions turn out to be correct...\")\n",
    "for df_type in [spx_1dte_dfs, spx_7dte_dfs, spx_28dte_dfs]:\n",
    "    s = 0\n",
    "    for df in df_type:\n",
    "        expiry_date = str(pd.to_datetime(df['expire_unix'].iloc[0], unit=\"s\").date())\n",
    "\n",
    "        strikes_fine, bs_prices_fine, f_2 = create_pdf_from_df(df, 'call')\n",
    "        # max_probability_price = minimize_scalar( lambda x: -f_2(x), bounds=(strikes_fine.min(), strikes_fine.max()), method='bounded').x\n",
    "        densities = f_2(strikes_fine)\n",
    "        max_idx = np.argmax(densities)\n",
    "        max_probability_price = strikes_fine[max_idx] \n",
    "        score = crps(f_2, max_probability_price, strikes_fine.min(), strikes_fine.max())\n",
    "        # if score > 150:\n",
    "        #     print(f\"Maybe some bad data in {df['dte'].iloc[0]} DTE calls quoted on {df['quote_time']}\")\n",
    "        #     plt.plot(strikes_fine, f_2(strikes_fine))\n",
    "        #     plt.vlines(max_probability_price, ymin=0, ymax=.1, color=\"#777777\", linestyle=\"--\")\n",
    "        #     plt.title(\"probability distribution\")\n",
    "        #     plt.show()\n",
    "        #     plt.plot(strikes_fine, bs_prices_fine)\n",
    "        #     plt.title(\"smoothed prices\")\n",
    "        #     plt.show()\n",
    "        s += score                             \n",
    "\n",
    "    print(f\"Average CRPS for {int(df_type[0]['dte'].iloc[0])} DTE calls if all predictions were correct: {s / len( df_type ) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ff52a",
   "metadata": {},
   "source": [
    "# Part 2: converting the risk-neutral density to a real-world density\n",
    "\n",
    "As discussed before, the density that we are extracting from the option prices is not just purely the market's prediction of where the stock is likely to move.\n",
    "So far I have obtained a density $f_Q$ for the risk-neutral probability measure $Q$, and I would like to convert this to a density $f_P$ for the real-world probability measure $P$.\n",
    "In general there is not an obvious way to do this.\n",
    "\n",
    "Recall our previous calculation that\n",
    "\n",
    "\\begin{align*}\n",
    "C(K, T) &= e^{-rT} \\mathbb E_Q(\\max\\{S_T - K, 0\\}) \\\\\n",
    "&= e^{-rT} \\int_0^\\infty \\max\\{x - K, 0\\} f_Q(x) \\, d x\n",
    "\\end{align*}\n",
    "We can continue this:\n",
    "\\begin{align*}\n",
    "\\qquad \\qquad \\qquad \\quad &= e^{-rT} \\int_0^\\infty \\max\\{x - K, 0\\} \\frac{f_Q(x)}{f_P(x)} f_P(x) \\, dx \\\\\n",
    "&= \\mathbb E_P[ \\max\\{S_T - K, 0\\} \\cdot m(S_T) ],\n",
    "\\end{align*}\n",
    "where $m(S_T)$ is a random variable called the *stochastic discount factor*, and is defined by the equation above, so that\n",
    "$$\n",
    "m(S_T) = e^{-r T} \\frac{f_Q(S_T)}{f_P(S_T)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346770ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def physical_density_from_rn(fQ, s_min, s_max, n_grid=500, gamma=4):\n",
    "    \"\"\"\n",
    "    Convert a callable risk-neutral pdf fQ(s) into a callable physical pdf fP(s)\n",
    "    under a CRRA/power-utility assumption with coefficient gamma.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fQ : callable\n",
    "        Function fQ(s): risk-neutral pdf (should integrate ≈ 1 over its support).\n",
    "    s_min, s_max : float\n",
    "        Bounds of the support (integration domain).\n",
    "    n_grid : int, optional\n",
    "        Number of grid points used for numerical integration and interpolation (default 500).\n",
    "    gamma : float, optional\n",
    "        Coefficient of relative risk aversion (default 4).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fP_func : callable\n",
    "        Function fP(s): normalized physical (real-world) pdf.\n",
    "    s_grid : ndarray\n",
    "        Grid used for normalization and interpolation.\n",
    "    fP_grid : ndarray\n",
    "        Values of fP(s) on that grid.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    f_P(s) = (s^gamma / E_Q[S_T^gamma]) * f_Q(s)\n",
    "    \n",
    "    \"\"\"\n",
    "    # evaluate fQ on a grid\n",
    "    s_grid = np.linspace(s_min, s_max, n_grid)\n",
    "    fQ_grid = fQ(s_grid)\n",
    "    fQ_grid = np.clip(fQ_grid, 0, None)  # remove tiny negatives\n",
    "\n",
    "    # compute E_Q[S_T^gamma]\n",
    "    Z = np.trapezoid((s_grid ** gamma) * fQ_grid, s_grid)\n",
    "\n",
    "    fP_grid = (s_grid ** gamma) * fQ_grid / Z\n",
    "\n",
    "    # normalize\n",
    "    norm = np.trapezoid(fP_grid, s_grid)\n",
    "    if norm > 0:\n",
    "        fP_grid /= norm\n",
    "\n",
    "    fP_func = CubicSpline(s_grid, fP_grid, extrapolate=False)\n",
    "\n",
    "    return fP_func, s_grid, fP_grid\n",
    "\n",
    "\n",
    "def evaluate_physical_option_prediction_from_df(df: pd.DataFrame, option_type):\n",
    "    \"\"\"\n",
    "    Given a dataframe, construct the risk-neutral implied pdf of option prices at the expiry date of the options in the frame.\n",
    "    Then convert to a physical probability.\n",
    "    Then look up the actual price of SPX at close on the day of expiration and use CRPS to score the prediction.\n",
    "    \"\"\"\n",
    " \n",
    "    expiry_date = str(pd.to_datetime(df['expire_unix'].iloc[0], unit=\"s\").date())\n",
    "    spx_price_on_expiry_date = spx_data.loc[expiry_date, \"Close\"].iloc[0]\n",
    "\n",
    "\n",
    "    strikes_fine, bs_prices_fine, f_2 = create_pdf_from_df(df, option_type)\n",
    "    fP, s_grid, fP_grid = physical_density_from_rn(f_2, strikes_fine.min(), strikes_fine.max())\n",
    "    \n",
    "    return crps(fP, spx_price_on_expiry_date, strikes_fine.min(), strikes_fine.max())\n",
    "\n",
    "\n",
    "\n",
    "# # test this out\n",
    "# for _ in range(10):\n",
    "#     df = random.choice(spx_28dte_dfs) # choose a random df\n",
    "#     strikes_fine, bs_prices_fine, f_2 = create_pdf_from_df(df, \"call\")\n",
    "#     fP, s_grid, fP_grid = physical_density_from_rn(f_2, strikes_fine.min(), strikes_fine.max())\n",
    "#     plt.plot(strikes_fine, f_2(strikes_fine))\n",
    "#     plt.plot(strikes_fine, fP(strikes_fine))\n",
    "#     plt.show()\n",
    "\n",
    "#     print(evaluate_rn_option_prediction_from_df(df, 'call'))\n",
    "#     print(evaluate_physical_option_prediction_from_df(df, 'call'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be507a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Starting to calculate CRPS for implied real-world probability distributions...\")\n",
    "for df_type in [spx_1dte_dfs, spx_7dte_dfs, spx_28dte_dfs]:\n",
    "# for df_type in [spx_1dte_dfs]:\n",
    "    c = 0\n",
    "    p = 0\n",
    "    for df in df_type:\n",
    "\n",
    "        c_score = evaluate_physical_option_prediction_from_df(df, 'call')\n",
    "        p_score = evaluate_physical_option_prediction_from_df(df, 'put')\n",
    "\n",
    "        c += c_score\n",
    "        p += p_score\n",
    "\n",
    "    print(f\"Average CRPS for implied physical probability densities of calls in {int(df_type[0]['dte'].iloc[0])}: {c_score / len( df_type )}\")\n",
    "    print(f\"Average CRPS for implied physical probability densities of puts in {int(df_type[0]['dte'].iloc[0])}: {p_score / len( df_type )}\")\n",
    "print(\"Done.\")\n",
    "    # print(f\"Average CRPS for implied physical probability densities of calls in {int(df_type[0]['dte'].iloc[0])}: {s / 10}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f9163",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "\n",
    "Here I have collected some sources of information for this project, as well as some interesting resources related to the topics discussed.\n",
    "\n",
    "### **Market-implied probabilities**\n",
    "\n",
    "[Are market implied probabilities useful?](https://blog.thinknewfound.com/2017/11/market-implied-probabilities-useful/) - A blog post.\n",
    "\n",
    "[A blog post titled \"Options Implied Distributions are NOT Real-World Distributions\"](https://freeportlogbook.substack.com/p/options-implied-distributions-are) - A blog post.\n",
    "\n",
    "### **Estimating real-world probabilities from option-implied risk-neutral probabilities**\n",
    "\n",
    "[Understanding the Connection between Real-World and Risk-Neutral Scenario Generators.](https://www.soa.org/4ab88e/globalassets/assets/files/resources/research-report/2022/understanding-the-connection.pdf) - an SOA Research Institute article.\n",
    "\n",
    "[Steve Ross: The Recovery Theorem.](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12092) - A theorem about recovering the real-world probability distribution from the market-implied one.\n",
    "\n",
    "[Does the Ross recovery theorem work empirically?](https://www.sciencedirect.com/science/article/abs/pii/S0304405X20300763) - An article that gives some evidence that the Ross recovery theorem may break down in the real world.\n",
    "\n",
    "[Estimating the real-world density from the option-implied risk-neutral density.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2093397) - A paper.\n",
    "\n",
    "[Quant stackexchange.](https://quant.stackexchange.com/questions/50700/what-is-the-connection-between-the-risk-neutral-implied-density-and-the-real-wor/50701#50701) - A post about estimating the implied real-world density from option prices.\n",
    "\n",
    "\n",
    "[Bakshi, Kapadia, and Madan (2003) risk-neutral moment estimators: A Gram–Charlier density approach](https://link.springer.com/article/10.1007/s11147-022-09187-x)\n",
    "\n",
    "### **Other topics**\n",
    "\n",
    "[Wikipedia article on scoring rules](https://en.wikipedia.org/wiki/Scoring_rule).\n",
    "These are rules that are used to assign a score to a predicted probability distribution and an observed value; exactly what we are doing to compare the risk-neutral distributions to real market movements.\n",
    "\n",
    "\n",
    "[Stephen Taylor: Asset Prices Dynamics, Volatility, and Prediction.](https://ia801604.us.archive.org/20/items/quant_books/Asset%20Price%20Dynamics%2C%20Volatility%20_%20Prediction%20-%20S.%20J.%20Taylor.pdf) - A really good book about a lot of different topics related to this project.\n",
    "In particular see chapter 16: Density Prediction for Asset Prices, and section 16.9: From Risk-Neutral to Real-World Densities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
